# Project: Dynamic news clustering and recommendation system using sentence embeddings
**Urls for UIs of the service:**
- [Static GitHub pages 4.1](https://felixfritzen.github.io/Scalable_proj/)
- [Interaktive Huggingface space 4.2]( https://huggingface.co/spaces/felixfritzen/project)
---

**Implementation:**
- **Backfill pipline that is run once:** `1_feature_backfill.ipynb`
- **Daily updataes of headlines and embeddings:** `2_feature_pipeline.ipynb`
- **Training of K-means and PCA model:** `3_training_pipeline.ipynb`
- **Github pages visualization of clusters:** `4.1_interference.ipynb`
- **Huggingface space with headline prediction service:** `4.1_interference.ipynb`
---


## Development links
- [Vertex AI instans](https://console.cloud.google.com/vertex-ai/workbench/instances)
- [Code for the project](https://github.com/felixfritzen/Scalable_proj)

## Report
The aim of this project is to implement clustering and feature extraction to aid classification of news to uncover patterns and trends in news data. The goal was to create a platform that could cluster similar news items and provide meaningful insights into their relationships allowing users to obtain personalized recommendations of news articles based on their needs.
 We designed a pipeline that leverages both historical data and real-time updated data. The data is processed to create clusters and effectively group similar news headlines, enabling the identification of patterns and trends across different time periods. These clusters provide a foundation for both visualizing relationships within the data and offering personalized recommendations. We want to create a reliable yet intuitive user experience that includes a static visualization for understanding clustering patterns and a dynamic inference service for personalized article recommendations
The dataset that we used for this project consists of news headlines, with the primary sources being a Kaggle dataset and a live API from the BBC. The Kaggle dataset contains one year’s worth of historical BBC news headlines which provides us with a large and diverse foundation for training our models and developing the initial clustering framework. Given that BBC is one of the largest new sites in the world with many publications every day, using one year’s worth of data is enough. Furthermore, given that the topics relevant for news coverage changes rapidly, including older data than this might not provide any additional information that guide today’s news coverage. These headlines data is concise and highly informative which makes them ideal for extracting meaningful patterns and trends. The Kaggle dataset is structured to include fields that contain the headline text and the corresponding date which allows us to perform temporal clustering and analyze how trends evolve over time. This historical data serves as the backfill for creating a robust feature pipeline that ensures that our clustering models are trained on a comprehensive dataset that captures the nuances of headline language across various topics and time periods.

In addition to the historical data, using the BBC API (bbc-api.vercel.app) provides us with a daily stream of updated news headlines. By using this dynamic source, we are able to continuously incorporate the latest information which ensures that the clusters remain relevant and reflect current events.
Looking at the method more specifically, the pipeline starts with a feature backfill process, leveraging historical data from the Kaggle dataset of BBC news headlines. This step involves preprocessing and structuring the data so that we can easily use it for downstream tasks. The backfill process ensures that our models have access to data covering an entire year of news headlines. The data consists of the title of the article as well as the publication date. We limit the data to articles from the previous one year period. Before we move on to the feature pipeline process, the processed data is uploaded to Hopsworks, an online platform designed for managing and operationalizing machine learning workflows. By uploading the processed data to Hopsworks, we can easily access the data for subsequent stages of the pipeline. By uploading the data to a feature store in Hopsworks, we facilitate feature management and ensure consistency between training and inference.
 Once the historical data is processed, the next step in the pipeline is the feature pipeline, which integrates new, daily headlines fetched from the BBC API into the historical data that we fetched in the previous step. We also ensure that we do not obtain any duplicates of the data in this step of the process since duplicates could distort the dataset by over-representing specific entries.
 
 At this point, all the necessary data has been fetched and preprocessed and we are ready to train our model. This is done in the training pipeline. The first step in the training pipeline involves generating vector embeddings for the text data. We use a sentence transformer model, a type of deep learning architecture optimized for converting textual data into high-dimensional vector representations. This allows us to transform the raw headline text into numerical embeddings that capture semantic relationships and contextual meanings. These embeddings serve as the foundation for clustering  which we in turn use to identify patterns and trends in the data. Once the embeddings are generated, we apply the K-means clustering algorithm to the embeddings. This algorithm groups the embeddings into clusters based on their proximity in the embedding space that we created. The number of clusters is chosen experimentally and is a hyperparameter that we optimize. The clustering process enables the system to organize the news headlines into distinct categories where each represents a unique pattern or trend in the data. Since the clustering is done in a high-dimensional embedding space we look for a method to reduce the high-dimensional embeddings to a lower-dimensional space. This is done with PCA and allows us to project the clusters onto a 2D plane. The last step of the training step in the pipeline includes registering the model in the Hopsworks model registry so that we can easily access the model in downstream tasks. 
 
The last step of the pipeline consists of an inference pipeline which allows users to interact with the system dynamically. It consists of two distinct parts. The first part is static and operates without any user input. It uses the results of the K-means clustering and PCA dimensionality reduction from the training pipeline to display the clusters in a visually interpretable form. The pipeline maps the preprocessed and embedded dataset onto a 2D space (using PCA) and visualizes the clusters. Each cluster represents a grouping of semantically similar headlines, giving an overview of dominant news topics and their relationships. The second part involves direct user interaction. Here, the user provides a headline or text as input, and the system generates article recommendations based on  the input's semantic similarity to the pre-existing dataset. The input is first processed and transformed in a manner consistent with the training pipeline, including cleaning and embedding generation using the same sentence transformer model used during training. The generated embedding is passed to the trained K-means clustering model, which assigns the input headline to one of the predefined clusters. This classification provides immediate insight into the type of news content the headline relates to and its association with existing trends.  Once the input headline is assigned to a cluster, the system retrieves other headlines from the same cluster. These recommendations represent articles that are semantically similar to the input and provide the user with a selected list of related news items. We deploy the inference pipeline on Hugging Face Spaces, a platform that supports interactive machine learning applications. This deployment allows the user to interact with the system via a user-friendly interface. 

